{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b994bcfb-aed7-4633-84e8-3c018e773790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as torchdata\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from timeit import default_timer as time\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Bernoulli\n",
    "import utils\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a796f91-8214-4841-a8da-8ce3ed660264",
   "metadata": {},
   "source": [
    "# All variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e825118-d30a-4ea3-b4f6-c4deeea728a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"R110_C10\"\n",
    "parallel = False #set true if in notebook with multicore GPU\n",
    "alpha = 0.8\n",
    "beta = 1e-1\n",
    "lr = 1e-4\n",
    "penalty = -1\n",
    "batch_size = 256 #recommended: 2048\n",
    "max_epochs_training = 100 #original blockdrop: 10000\n",
    "max_epochs_finetuning = 20 #original blockdrop: 2000\n",
    "wd = 0.0\n",
    "cl_step = 1\n",
    "cv_dir_training = 'cv/trained_policy/'+ model \n",
    "cv_dir_finetuning = 'cv/finetuned/'+ model \n",
    "start_epoch = 0\n",
    "\n",
    "load = None\n",
    "data_dir = 'data/'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed01b5-2d6d-4421-b4f5-1db488fde5a6",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a295a-2443-42f0-b222-2c352891ebd9",
   "metadata": {},
   "source": [
    "## Reward calculation\n",
    "Reward calculation is done per batch (2048 images). All values are calculated within the tensor. However, the comments present the calculation for one image\n",
    "\n",
    "Inputs: preds (predicted class), targets (real class labels), policy()\n",
    "\n",
    "Really per complete batch!!!!!!!!!!!! len(match) = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc0d571-3abe-4f54-98ab-448abccf7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(preds, targets, policy):\n",
    " \n",
    "    block_use = policy.sum(1).float()/policy.size(1)   # no. of blocks used / all blocks for complete batch\n",
    "    sparse_reward = 1.0-block_use**2     # reward multiplicator -> the less blocks used, the higher the multiplicator\n",
    "\n",
    "    _, pred_idx = preds.max(1) #get predicted classes \n",
    "    match = (pred_idx==targets).data #get matrix of all correct predicted images\n",
    "    reward = sparse_reward\n",
    "    reward[torch.logical_not(match)] = penalty #all not correct predicted images get penalty\n",
    "    reward = reward.unsqueeze(1) \n",
    "    \n",
    "    #reward = performance-based reward for Policy Net; match = all correct predicted images \n",
    "    return reward, match.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc71422-a6cd-48cf-a23a-0bc75fe71afa",
   "metadata": {},
   "source": [
    "## Testing at end of training steps:\n",
    "At the end of the Policy training and finetuning, BlockDrop is tested once before saving the model for later use\n",
    "1. input image in policy and calculate output (probability vector of keeping/ dropping each layer)\n",
    "2. for each value of vector: if >0.5: 1 (keep), else : 0 (drop)\n",
    "3. forward pass of ResNet with only layers which are kept \n",
    "4. evaluate result\n",
    "5. save the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01458471-81bb-4464-9cb1-a79fdad20973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, cv_dir):\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    matches, rewards, policies = [], [], []\n",
    "    for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(testloader), total=len(testloader)):\n",
    "        \n",
    "        # 1. input image in policy and calculate output (probability vector of keeping/ dropping each layer)\n",
    "        targets = targets.to(device, non_blocking=True) \n",
    "\n",
    "        if not parallel:\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        probs, _ = agent(inputs)\n",
    "        \n",
    "        # 2. for each value of vector: if >0.5: 1 (keep), else : 0 (drop)\n",
    "        policy = probs.data.clone()\n",
    "        policy[policy<0.5] = 0.0\n",
    "        policy[policy>=0.5] = 1.0\n",
    "        \n",
    "        # 3. if still in curriculum learning: first 1-x layers are 1 anyway\n",
    "        if cl_step < num_blocks:\n",
    "            policy[:, :-cl_step] = 1\n",
    "        \n",
    "        # 4. forward pass of ResNet with only layers which are kept\n",
    "        preds = rnet.forward(inputs, policy)\n",
    "        \n",
    "        # preparation of evaluation  \n",
    "        reward, match = get_reward(preds, targets, policy.data)\n",
    "\n",
    "        matches.append(match)\n",
    "        rewards.append(reward)\n",
    "        policies.append(policy.data)\n",
    "\n",
    "    # 5. evaluate result\n",
    "    accuracy, reward, sparsity, variance, policy_set = utils.performance_stats(policies, rewards, matches)\n",
    "\n",
    "    log_str = 'TS - Accuracy: %.3f | Reward: %.2E | Sparsity: %.3f | Variance: %.3f | #: %d'%(accuracy, reward, sparsity, variance, len(policy_set))\n",
    "    print(log_str)\n",
    "    \n",
    "    # 6. save the model for future use\n",
    "    agent_state_dict = agent.module.state_dict() if parallel else agent.state_dict()\n",
    "\n",
    "    state = {\n",
    "      'agent': agent_state_dict,\n",
    "      'epoch': epoch,\n",
    "      'reward': reward,\n",
    "      'acc': accuracy\n",
    "    }\n",
    "    torch.save(state, cv_dir+'/ckpt_E_%d.t7'%(epoch))\n",
    "    pretrained = cv_dir+'/ckpt_E_%d.t7'%(epoch)\n",
    "    print(\"Model saved: \", pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724166-e206-402c-a2aa-05eecaf08872",
   "metadata": {},
   "source": [
    "# Policy Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10796e9-a8dc-4e6f-a691-854489d2e6fc",
   "metadata": {},
   "source": [
    "## Training script\n",
    "For each batch:\n",
    "1. run model on input data to get current stand of policy\n",
    "2. for each value of vector: if >0.5: 1 (keep), else : 0 (drop)\n",
    "3. create probs_new (= a new random sample) for the dropping strategy --> goal: compare if it is better or worse than current Policy\n",
    "4. if still in curriculum learning: in first x (= num_blocks of ResNet) epochs: first 1-x layers are set to 1 of both, probs_new and probs\n",
    "5. forward pass of ResNet with only layers which are kept for both dropping strategies\n",
    "6. calculate rewards for both rnets and compare them (= advantage)\n",
    "7. calculate loss and backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa4cdf3-558d-419b-8c20-ccd63f0b4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "   \n",
    "    agent.train()\n",
    "\n",
    "    matches, rewards, policies = [], [], []\n",
    "    \n",
    "    # training for one one batch\n",
    "    for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "        \n",
    "        #move input and target data to the device\n",
    "        targets = targets.to(device, non_blocking=True) \n",
    "        if not parallel:\n",
    "            inputs = inputs.to(device)\n",
    "        \n",
    "        #run the model on the input data to get current stand of policy: probs = result of Policy, _ not used for this model\n",
    "        probs, _ = agent(inputs)\n",
    "\n",
    "        #---------------------------------------------------------------------#\n",
    "        # results of Policy Net are taken to create dropping strategy for ResNet: <0.5 drop; >=0.5: keep\n",
    "        policy_map = probs.data.clone()\n",
    "        policy_map[policy_map<0.5] = 0.0\n",
    "        policy_map[policy_map>=0.5] = 1.0\n",
    "        \n",
    "        #now create a sample: new dropping recommendations modified from the first one by using the alpha value  \n",
    "        # alpha: parameter to bound the distribution and prevent it from saturating (Paper, p. 8820)\n",
    "        probs_new = probs*alpha + (1-probs)*(1-alpha)\n",
    "        # bernoulli distribution to set get the dropping strategy (0= drop or 1= keep)\n",
    "        distr = Bernoulli(probs_new)\n",
    "        policy_sample = distr.sample()\n",
    "\n",
    "        #curriculum learning: in the first x (< than blocks of Resnet) iterations only the last x (num_blocks-iteration) layers\n",
    "        # of the ResNet are used for training the PolicyNet\n",
    "        # = only decisions for the last x blocks are saved; first 1-x layers are kept \n",
    "        if cl_step < num_blocks:\n",
    "            policy_sample[:, :-cl_step] = 1\n",
    "            policy_map[:, :-cl_step] = 1\n",
    "            \n",
    "            #policy_mask: set a new variable where only currently trained layers are set to 1; first 1-x are set to 0\n",
    "            policy_mask = torch.ones(inputs.size(0), policy_sample.size(1)).to(device)\n",
    "            policy_mask[:, :-cl_step] = 0\n",
    "        else:\n",
    "            policy_mask = None\n",
    "        \n",
    "        v_inputs = inputs.data      \n",
    "        # rnet with blocks recommended by current Policy\n",
    "        preds_map = rnet.forward(v_inputs, policy_map)\n",
    "        \n",
    "        # rnet blocks recommended by the sample (modified version based on Bernoulli)\n",
    "        preds_sample = rnet.forward(v_inputs, policy_sample)\n",
    "\n",
    "        # calculate reward for results of both rnets\n",
    "        reward_map, _ = get_reward(preds_map, targets, policy_map.data) # = baseline estimate (result of dropping strategy of current policy network)\n",
    "\n",
    "        reward_sample, match = get_reward(preds_sample, targets, policy_sample.data) #= reward (result of sample) \n",
    "        \n",
    "        # advantage --> calculates if sample is better than current strategy (>0) or worse (<0)\n",
    "        advantage = reward_sample - reward_map\n",
    "        \n",
    "        # if sample is better, the probabilites of the modified gradients are increased (as current*advantage >0 else: decreased\n",
    "        loss = -distr.log_prob(policy_sample)\n",
    "        loss = loss * advantage.expand_as(policy_sample) #x.expand_as(y): expand x to size of y\n",
    "        \n",
    "        # in case of curriculum learning stage: only loss for the currently modified x last layers is saved by discarding others\n",
    "        if policy_mask is not None:\n",
    "            loss = policy_mask * loss \n",
    "\n",
    "        loss = loss.sum()\n",
    "\n",
    "        probs_new = probs_new.clamp(1e-15, 1-1e-15) #clamp(min, max) -> size values to fit between min and max\n",
    "        entropy_loss = -probs_new*torch.log(probs_new)\n",
    "        entropy_loss = beta*entropy_loss.sum()\n",
    "\n",
    "        loss = (loss - entropy_loss)/inputs.size(0)\n",
    "\n",
    "        #---------------------------------------------------------------------#\n",
    "        #1. set gradients list to zero (delete gradients of former epoch), 2.do backpropagation, 3.update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # save results in list for performance evaluation of this epoch \n",
    "        matches.append(match.cpu())\n",
    "        rewards.append(reward_sample.cpu())\n",
    "        policies.append(policy_sample.data.cpu())\n",
    "    \n",
    "    #calculate performance metrics for this epoch\n",
    "    accuracy, reward, sparsity, variance, policy_set = utils.performance_stats(policies, rewards, matches)\n",
    "\n",
    "    result_train = 'Epoch: %d - Accuracy: %.3f | Reward: %.2E | Sparsity: %.3f | Variance: %.3f | #: %d'%(epoch, accuracy, reward, sparsity, variance, len(policy_set))\n",
    "\n",
    "    print(result_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb1340-655d-407b-98bf-74dc753c37e5",
   "metadata": {},
   "source": [
    "## Run Policy Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b296e86e-01c1-4666-b0ff-35ce2843f902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the last 1 blocks ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:18<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Accuracy: 0.993 | Reward: 1.89E-02 | Sparsity: 53.277 | Variance: 0.448 | #: 2\n",
      "training the last 2 blocks ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [03:12<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Accuracy: 0.992 | Reward: 4.59E-02 | Sparsity: 52.514 | Variance: 0.618 | #: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:18<00:00, 33.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS - Accuracy: 0.930 | Reward: -2.17E-03 | Sparsity: 52.000 | Variance: 0.000 | #: 1\n",
      "Model saved:  cv/trained_policy/R110_C10/ckpt_E_1.t7\n",
      "Training in s: 414.60\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = utils.get_dataset(model, data_dir)\n",
    "num_workers = 4 if torch.device(\"cuda\") else 1\n",
    "\n",
    "trainloader = torchdata.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers) #50000 images\n",
    "testloader = torchdata.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers) #10000 images\n",
    "rnet, agent = utils.get_model(model, device)\n",
    "num_blocks = sum(rnet.layer_config)\n",
    "\n",
    "if load is not None:\n",
    "    checkpoint = torch.load(load)\n",
    "    agent.load_state_dict(checkpoint['agent'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print('loaded agent from', load)\n",
    "\n",
    "if parallel:\n",
    "    agent = nn.DataParallel(agent)\n",
    "    rnet = nn.DataParallel(rnet)\n",
    "\n",
    "if torch.device(\"cuda\"):\n",
    "    rnet.eval().cuda()\n",
    "    agent.cuda()\n",
    "elif torch.device(\"cpu\"):\n",
    "    rnet.eval().to(device)\n",
    "    agent.to(device)\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "start_training = timer()\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+max_epochs_training+1):\n",
    "\n",
    "    if cl_step < num_blocks:\n",
    "        cl_step = 1 + 1 * (epoch // 1)\n",
    "    else:\n",
    "        cl_step = num_blocks\n",
    "\n",
    "    print('training the last %d blocks ...' % cl_step)\n",
    "\n",
    "    train(epoch)\n",
    "\n",
    "    # the testing is done after last epoch\n",
    "    if epoch > 0 and epoch % max_epochs_training == 0:\n",
    "        pretrained = test(epoch, cv_dir_training)\n",
    "\n",
    "end_training = timer()\n",
    "training_time = end_training -start_training\n",
    "print(\"Training in s: %.2f\"%(training_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c057c4-6104-43e2-bf62-18cf5f86a45e",
   "metadata": {},
   "source": [
    "# Joint Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b803461-c3ab-410f-9436-3ad639185e13",
   "metadata": {},
   "source": [
    "## Training of finetuning\n",
    "same as in train.ipynb, but also training of rnet:\n",
    "\n",
    "For each batch:\n",
    "1. run model on input data to get current stand of policy\n",
    "2. for each value of vector: if >0.5: 1 (keep), else : 0 (drop)\n",
    "3. create probs_new (= a new random sample) for the dropping strategy --> goal: compare if it is better or worse than current Policy\n",
    "4. if still in curriculum learning: in first x (= num_blocks of ResNet) epochs: first 1-x layers are set to 1 of both, probs_new and probs\n",
    "5. forward pass of ResNet with only layers which are kept for both dropping strategies\n",
    "6. calculate rewards for both rnets and compare them (= advantage)\n",
    "7. calculate loss and backpropagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff30bd0a-d1a9-4acf-8175-aab80284e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(epoch):\n",
    "\n",
    "    agent.train()\n",
    "    rnet.train()\n",
    "\n",
    "    matches, rewards, policies = [], [], []\n",
    "\n",
    "    # training for one one batch\n",
    "    for batch_idx, (inputs, targets) in tqdm.tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "\n",
    "        #move input and target data to the device\n",
    "        targets = targets.to(device, non_blocking=True) \n",
    "        if not parallel:\n",
    "            inputs = inputs.to(device) \n",
    "        \n",
    "        #run the model on the input data to get current stand of policy: probs = result of Policy, _ not used for this model\n",
    "        probs, _ = agent(inputs)\n",
    "\n",
    "        #---------------------------------------------------------------------#\n",
    "\n",
    "        # results of Policy Net are taken to create dropping strategy for ResNet: <0.5 drop; >=0.5: keep\n",
    "        policy_map = probs.data.clone()\n",
    "        policy_map[policy_map<0.5] = 0.0\n",
    "        policy_map[policy_map>=0.5] = 1.0\n",
    "\n",
    "        #now create a sample: new dropping recommendations modified from the first one by using the alpha value  \n",
    "        # alpha: parameter to bound the distribution and prevent it from saturating (Paper, p. 8820)\n",
    "        probs = probs*alpha + (1-probs)*(1-alpha)\n",
    "        distr = Bernoulli(probs)\n",
    "        policy = distr.sample()\n",
    "\n",
    "        v_inputs = inputs.data\n",
    "\n",
    "        # rnet with blocks recommended by current Policy\n",
    "        preds_map = rnet.forward(v_inputs, policy_map)\n",
    "\n",
    "        # rnet blocks recommended by the sample (modified version based on Bernoulli)\n",
    "        preds_sample = rnet.forward(inputs, policy)\n",
    "\n",
    "        # calculate reward for results of both rnets\n",
    "        reward_map, _ = get_reward(preds_map, targets, policy_map.data)\n",
    "        reward_sample, match = get_reward(preds_sample, targets, policy.data)\n",
    "\n",
    "        # advantage --> calculates if sample is better than current strategy (>0) or worse (<0)\n",
    "        advantage = reward_sample - reward_map\n",
    "\n",
    "        # if sample is better, the probabilites of the modified gradients are increased (as current*advantage >0 else: decreased\n",
    "        loss = -distr.log_prob(policy).sum(1, keepdim=True) * advantage\n",
    "        loss = loss.sum()\n",
    "        loss += F.cross_entropy(preds_sample, targets)\n",
    "\n",
    "\n",
    "        #---------------------------------------------------------------------#\n",
    "        #1. set gradients list to zero (delete gradients of former epoch), 2.do backpropagation, 3.update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        matches.append(match.cpu())\n",
    "        rewards.append(reward_sample.cpu())\n",
    "        policies.append(policy.data.cpu())\n",
    "\n",
    "    #calculate performance metrics for this epoch\n",
    "    accuracy, reward, sparsity, variance, policy_set = utils.performance_stats(policies, rewards, matches)\n",
    "\n",
    "    result_finetune = 'Epoch: %d - Accuracy: %.3f | Reward: %.2E | Sparsity: %.3f | Variance: %.3f | #: %d'%(epoch, accuracy, reward, sparsity, variance, len(policy_set))\n",
    "\n",
    "    print(result_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b828462-e040-433a-bb72-d0e278d249ba",
   "metadata": {},
   "source": [
    "## Run Joint Finetuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53ee5b5-c900-4f15-acca-5150010e4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [06:29<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Accuracy: 0.754 | Reward: 2.67E-01 | Sparsity: 29.854 | Variance: 3.714 | #: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [06:23<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Accuracy: 0.870 | Reward: 4.63E-01 | Sparsity: 30.099 | Variance: 3.558 | #: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:21<00:00, 29.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS - Accuracy: 0.891 | Reward: -5.02E-02 | Sparsity: 52.179 | Variance: 0.383 | #: 2\n",
      "Model saved:  cv/finetuned/R110_C10/ckpt_E_1.t7\n",
      "Finetuning in s: 800.63\n"
     ]
    }
   ],
   "source": [
    "rnet, agent = utils.get_model(model, device)\n",
    "\n",
    "if pretrained is not None:\n",
    "    checkpoint = torch.load(pretrained)\n",
    "    key = 'net' if 'net' in checkpoint else 'agent'\n",
    "    agent.load_state_dict(checkpoint[key])\n",
    "    print('loaded pretrained model from', pretrained)\n",
    "\n",
    "if load is not None:\n",
    "    checkpoint = torch.load(load)\n",
    "    rnet.load_state_dict(checkpoint['resnet'])\n",
    "    agent.load_state_dict(checkpoint['agent'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print('loaded agent from', load)\n",
    "\n",
    "\n",
    "if parallel:\n",
    "    agent = nn.DataParallel(agent)\n",
    "    rnet = nn.DataParallel(rnet)\n",
    "\n",
    "rnet.to(device)\n",
    "agent.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(agent.parameters())+list(rnet.parameters()), lr=lr, weight_decay=wd)\n",
    "\n",
    "start_finetuning = timer()\n",
    "for epoch in range(start_epoch, start_epoch+max_epochs_finetuning+1):\n",
    "    \n",
    "    finetune(epoch)\n",
    "    # the testing is done after last epoch\n",
    "    if epoch > 0 and epoch % max_epochs_finetuning == 0:\n",
    "        test(epoch, cv_dir_finetuning)\n",
    "        \n",
    "end_finetuning = timer()\n",
    "finetuning_time = end_finetuning -start_finetuning\n",
    "print(\"Finetuning in s: %.2f\"%(finetuning_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0425e-3868-49d9-af26-eb93b1ec8bca",
   "metadata": {},
   "source": [
    "# Times of complete training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe8d15e-4e54-409f-ba71-e172b7c519da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Training: 414.60 | Finetuning: 800.63\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy Training: %.2f | Finetuning: %.2f\" %(training_time, finetuning_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
